# -*- coding: utf-8 -*-
"""Stanford.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yuUIDKOvTH1iToxqBr26pmepU8hxDDCp
"""

import os
import zipfile
from io import BytesIO
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import pandas as pd
from torchvision import transforms

# Определение устройства (GPU или CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

class PalmDataset(Dataset):
    def __init__(self, csv_file, zip_file, transform=None):
        self.labels = pd.read_csv(csv_file)
        self.transform = transform
        self.zip_file = zip_file

        # Открываем ZIP-файл
        self.archive = zipfile.ZipFile(zip_file, 'r')

        # Фильтрация данных только для ладоней
        self.labels = self.labels[self.labels['aspectOfHand'].str.contains('palmar', case=False, na=False)]

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # Получаем имя изображения
        img_name = self.labels.iloc[idx, 7]  # imageName
        img_path = f"Hands/{img_name}"  # Путь к файлу внутри архива, если все изображения в папке "images"

        # Извлекаем изображение из архива
        with self.archive.open(img_path) as img_file:
            img = Image.open(BytesIO(img_file.read())).convert("RGB")

        # Применяем трансформации, если они заданы
        if self.transform:
            img = self.transform(img)

        # Получаем метки
        age = self.labels.iloc[idx, 1]  # возраст
        skin_color = self.labels.iloc[idx, 3].lower()  # цвет кожи
        accessories = self.labels.iloc[idx, 4]  # наличие аксессуаров

        skin_color_mapping = {'very fair': 0,'fair': 1, 'medium': 2, 'dark': 3} #['fair' 'dark' 'medium' 'very fair']
        skin_color_label = skin_color_mapping.get(skin_color, -1)

        # Преобразование меток в числовой формат (например, one-hot)
        label = torch.tensor([age, skin_color_label, accessories], dtype=torch.float32)

        return img, label

# Путь к CSV файлу с метками и путь к ZIP-файлу с изображениями
csv_file = 'content/dataset/HandInfo.csv'
zip_file = 'content/dataset/images/Hands.zip'

df = pd.read_csv(csv_file)
print(df.columns)

# Определяем преобразования для изображений
transform = transforms.Compose([
    transforms.Resize((512, 512)),  # Изменение размера изображения до 512x512
    transforms.ToTensor(),  # Преобразование изображения в тензор
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Нормализация
])

# Создаем датасет с использованием класса PalmDataset
dataset = PalmDataset(csv_file=csv_file, zip_file=zip_file, transform=transform)

# Создаем DataLoader
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)


class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, condition_dim):
        super(ConditionalGenerator, self).__init__()

        self.init_size = 8  # Начальный размер после первого слоя (например, 8x8)
        self.fc = nn.Sequential(
            nn.Linear(latent_dim + condition_dim, 128 * self.init_size ** 2)  # Соединяем шум и условие
        )

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Увеличение до 16x16
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Увеличение до 32x32
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Увеличение до 64x64
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),  # Увеличение до 128x128
            nn.Tanh()  # Приведение к диапазону значений [-1, 1]
        )

    def forward(self, z, condition):
        x = torch.cat([z, condition], dim=1)
        print(f"Размерность после объединения: {x.shape}")
        out = self.fc(x)
        print(f"Размерность после fully connected: {out.shape}")
        out = out.view(out.size(0), 128, self.init_size, self.init_size)
        print(f"Размерность после reshape: {out.shape}")
        img = self.conv_blocks(out)
        print(f"Размерность изображения: {img.shape}")
        return img

class Discriminator(nn.Module):
    def __init__(self, condition_dim):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Conv2d(3 + condition_dim, 16, kernel_size=4, stride=2, padding=1),  # Уменьшение до 64x64
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),  # Уменьшение до 32x32
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Уменьшение до 16x16
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Уменьшение до 8x8
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=0),  # Окончательный вывод
            nn.Sigmoid()
        )

    def forward(self, img, condition):
        # Конкатенируем изображение и условие по канальному измерению
        condition = condition.unsqueeze(2).unsqueeze(3)  # Изменяем размерность условия для конкатенации
        print(f"Размерность условия для конкатенации: {condition}")
        condition = condition.expand(condition.size(0), condition.size(1), img.size(2), img.size(3))
        x = torch.cat([img, condition], dim=1)  # Условие и изображение вместе
        validity = self.model(x)
        return validity.view(-1, 1)

# Параметры GAN
latent_dim = 100  # Размер шумового вектора
condition_dim = 3  # Возраст, цвет кожи, аксессуары
lr = 0.0002
num_epochs = 100

# Инициализация модели
generator = ConditionalGenerator(latent_dim, condition_dim).to(device)
discriminator = Discriminator(condition_dim).to(device)

# Оптимизаторы
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)

# Функция потерь
adversarial_loss = nn.BCEWithLogitsLoss()

# Тренировочный цикл
for epoch in range(num_epochs):
    for i, (real_imgs, labels) in enumerate(dataloader):

        # Метки для настоящих и фейковых изображений
        valid = torch.ones(real_imgs.size(0), 1).to(device)
        fake = torch.zeros(real_imgs.size(0), 1).to(device)

        # Настоящие изображения
        real_imgs = real_imgs.to(device)
        labels = labels.to(device)  # Метки: возраст, цвет кожи, аксессуары

        # === Тренировка дискриминатора ===

        # Генерация шума и фейковых изображений
        z = torch.randn(real_imgs.size(0), latent_dim).to(device)
        gen_imgs = generator(z, labels)

        # Рассчитываем потери дискриминатора на реальных и фейковых изображениях
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        # Обновляем дискриминатор
        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # === Тренировка генератора ===

        # Теперь дискриминатор оценивает фейковые изображения как "настоящие"
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        # Обновляем генератор
        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    print(f"Epoch {epoch}/{num_epochs} | D loss: {d_loss.item()} | G loss: {g_loss.item()}")