# -*- coding: utf-8 -*-
"""Stanford.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yuUIDKOvTH1iToxqBr26pmepU8hxDDCp
"""

import os
import zipfile
from io import BytesIO
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import pandas as pd
from torchvision import transforms

# Определение устройства (GPU или CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

class PalmDataset(Dataset):
    def __init__(self, csv_file, zip_file, transform=None):
        self.labels = pd.read_csv(csv_file)
        self.transform = transform
        self.zip_file = zip_file

        # Открываем ZIP-файл
        self.archive = zipfile.ZipFile(zip_file, 'r')

        # Фильтрация данных только для ладоней
        self.labels = self.labels[self.labels['aspectOfHand'].str.contains('palmar', case=False, na=False)]

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # Получаем имя изображения
        img_name = self.labels.iloc[idx, 7]  # imageName
        img_path = f"Hands/{img_name}"  # Путь к файлу внутри архива, если все изображения в папке "images"

        # Извлекаем изображение из архива
        with self.archive.open(img_path) as img_file:
            img = Image.open(BytesIO(img_file.read())).convert("RGB")

        # Применяем трансформации, если они заданы
        if self.transform:
            img = self.transform(img)

        # Получаем метки
        age = self.labels.iloc[idx, 1]  # возраст
        skin_color = self.labels.iloc[idx, 3].lower()  # цвет кожи
        accessories = self.labels.iloc[idx, 4]  # наличие аксессуаров

        skin_color_mapping = {'very fair': 0, 'fair': 1, 'medium': 2, 'dark': 3}  # ['fair' 'dark' 'medium' 'very fair']
        skin_color_label = skin_color_mapping.get(skin_color, -1)

        # Преобразование меток в числовой формат (например, one-hot)
        label = torch.tensor([age, skin_color_label, accessories], dtype=torch.float32)

        return img, label

# Путь к CSV файлу с метками и путь к ZIP-файлу с изображениями
csv_file = 'content/dataset/HandInfo.csv'
zip_file = 'content/dataset/images/Hands.zip'

df = pd.read_csv(csv_file)
print(df.columns)

# Определяем преобразования для изображений
transform = transforms.Compose([
    transforms.Resize((512, 512)),  # Изменение размера изображения до 512x512
    transforms.ToTensor(),  # Преобразование изображения в тензор
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Нормализация
])

# Создаем датасет с использованием класса PalmDataset
dataset = PalmDataset(csv_file=csv_file, zip_file=zip_file, transform=transform)

# Создаем DataLoader
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)


class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, condition_dim):
        super(ConditionalGenerator, self).__init__()

        self.init_size = 8  # Начальный размер после fully-connected слоя
        self.fc = nn.Sequential(
            nn.Linear(latent_dim + condition_dim, 512 * self.init_size ** 2)  # Изменяем количество фильтров на 512
        )

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(512),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 16x16
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 32x32
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 64x64
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 128x128
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 256x256
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),  # Увеличиваем до 512x512
            nn.Tanh()  # Приведение к диапазону [-1, 1]
        )

    def forward(self, z, condition):
        x = torch.cat([z, condition], dim=1)
        out = self.fc(x)
        out = out.view(out.size(0), 512, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img


class Discriminator(nn.Module):
    def __init__(self, condition_dim):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            nn.Conv2d(3 + condition_dim, 32, kernel_size=4, stride=2, padding=1),  # 512x512 -> 256x256
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 256x256 -> 128x128
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128x128 -> 64x64
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1), # 16x16 -> 8x8
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(1024, 1, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4
        )

        # Добавляем адаптивное усреднение
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Преобразование до 1x1

    def forward(self, img, condition):
        # Конкатенируем изображение и условие по канальному измерению
        condition = condition.unsqueeze(2).unsqueeze(3)  # Изменяем размерность условия для конкатенации
        condition = condition.expand(condition.size(0), condition.size(1), img.size(2), img.size(3))
        x = torch.cat([img, condition], dim=1)  # Объединяем условие и изображение

        validity = self.model(x)  # Получаем выход из свертки
        validity = self.adaptive_pool(validity)  # Применяем адаптивное усреднение
        validity = validity.view(-1, 1)  # Преобразуем в размерность [batch_size, 1]

        return validity


# Параметры GAN
latent_dim = 100  # Размер шумового вектора
condition_dim = 3  # Возраст, цвет кожи, аксессуары
lr_G = 0.0001  # Уменьшение скорости обучения генератора
lr_D = 0.0004  # Увеличение скорости обучения дискриминатора
num_epochs = 100
lambda_gp = 10  # Параметр для градиентного штрафа

# Инициализация модели
generator = ConditionalGenerator(latent_dim, condition_dim).to(device)
discriminator = Discriminator(condition_dim).to(device)

# Оптимизаторы
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_G)
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_D)

# Функция потерь
def gradient_penalty(discriminator, real_imgs, fake_imgs, labels):
    """Функция градиентного штрафа."""
    alpha = torch.randn(real_imgs.size(0), 1, 1, 1).to(device)

    fake_imgs = nn.functional.interpolate(fake_imgs, size=real_imgs.shape[2:])

    interpolated_imgs = alpha * real_imgs + (1 - alpha) * fake_imgs
    interpolated_imgs.requires_grad_(True)

    validity = discriminator(interpolated_imgs, labels)
    gradients = torch.autograd.grad(outputs=validity, inputs=interpolated_imgs,
                                     grad_outputs=torch.ones(validity.size()).to(device),
                                     create_graph=True, retain_graph=True)[0]

    gradient_norm = gradients.view(gradients.size(0), -1).norm(2, dim=1)  # L2-норма
    return ((gradient_norm - 1) ** 2).mean()  # Вернем штраф

# Тренировочный цикл
for epoch in range(num_epochs):
    for i, (real_imgs, labels) in enumerate(dataloader):

        # Метки для настоящих и фейковых изображений
        valid = torch.ones(real_imgs.size(0), 1).to(device)
        fake = torch.zeros(real_imgs.size(0), 1).to(device)

        # Настоящие изображения
        real_imgs = real_imgs.to(device)
        labels = labels.to(device)  # Метки: возраст, цвет кожи, аксессуары

        # === Тренировка дискриминатора ===
        for _ in range(5):  # Увеличим количество шагов для дискриминатора
            # Генерация шума и фейковых изображений
            z = torch.randn(real_imgs.size(0), latent_dim).to(device)
            gen_imgs = generator(z, labels)

            # Рассчитываем потери дискриминатора на реальных и фейковых изображениях
            real_loss = -torch.mean(discriminator(real_imgs, labels))  # WGAN loss
            fake_loss = torch.mean(discriminator(gen_imgs.detach(), labels))  # WGAN loss
            d_loss = real_loss + fake_loss

            # Добавляем градиентный штраф
            gp = gradient_penalty(discriminator, real_imgs.data, gen_imgs.data, labels)
            d_loss += lambda_gp * gp

            # Обновляем дискриминатор
            optimizer_D.zero_grad()
            d_loss.backward()
            optimizer_D.step()

        # === Тренировка генератора ===
        g_loss = -torch.mean(discriminator(gen_imgs, labels))  # WGAN loss

        # Обновляем генератор
        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    print(f"Epoch {epoch}/{num_epochs} | D loss: {d_loss.item()} | G loss: {g_loss.item()}")
